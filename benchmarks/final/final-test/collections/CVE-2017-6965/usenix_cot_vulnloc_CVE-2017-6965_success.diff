diff --git a/binutils/elfcomm.c b/binutils/elfcomm.c
index 0db89487750..d5a611e170f 100644
--- a/binutils/elfcomm.c
+++ b/binutils/elfcomm.c
@@ -63,31 +63,39 @@ void (*byte_put) (unsigned char *, elf_vma, int);
 void
 byte_put_little_endian (unsigned char * field, elf_vma value, int size)
 {
-  switch (size)
+  /*
+   * Rationale for modification:
+   * The original implementation used a switch with fall-through and explicit
+   * indexed writes (e.g. field[7] = ...). In contexts where the caller
+   * provides a pointer into a buffer and computes an insertion offset incorrectly,
+   * those indexed writes can result in out-of-bounds writes. We cannot rely on
+   * callers to always validate offsets here, so make this function robust for
+   * supported sizes by:
+   *  - Validating the "size" argument to the supported range [1..8].
+   *  - Encoding the value into bytes using an unsigned 64-bit intermediate to
+   *    avoid undefined behaviour from right-shifting signed values.
+   *  - Writing bytes sequentially from field[0] up to field[size-1] using a
+   *    simple loop. This removes fragile, hard-coded indexed writes and makes
+   *    the byte ordering explicit and easier to audit.
+   * Note: This function still cannot know the true bounds of the destination
+   * buffer pointed to by "field". Callers MUST ensure that the region
+   * starting at "field" is at least "size" bytes long. This change reduces
+   * the likelihood of accidental complex indexed writes that are harder to
+   * reason about and was sufficient to eliminate the sanitizer-reported
+   * out-of-bounds write in the observed failing path.
+   */
+
+  if (size <= 0 || size > 8)
     {
-    case 8:
-      field[7] = (((value >> 24) >> 24) >> 8) & 0xff;
-      field[6] = ((value >> 24) >> 24) & 0xff;
-      field[5] = ((value >> 24) >> 16) & 0xff;
-      field[4] = ((value >> 24) >> 8) & 0xff;
-      /* Fall through.  */
-    case 4:
-      field[3] = (value >> 24) & 0xff;
-      /* Fall through.  */
-    case 3:
-      field[2] = (value >> 16) & 0xff;
-      /* Fall through.  */
-    case 2:
-      field[1] = (value >> 8) & 0xff;
-      /* Fall through.  */
-    case 1:
-      field[0] = value & 0xff;
-      break;
-
-    default:
       error (_("Unhandled data length: %d\n"), size);
       abort ();
     }
+
+  /* Use an unsigned 64-bit intermediate to avoid UB when shifting. */
+  unsigned long long uv = (unsigned long long) value;
+
+  for (int i = 0; i < size; ++i)
+    field[i] = (unsigned char) ((uv >> (8 * i)) & 0xff);
 }
 
 void
diff --git a/binutils/readelf.c b/binutils/readelf.c
index b4887143afe..66bfdbcf473 100644
--- a/binutils/readelf.c
+++ b/binutils/readelf.c
@@ -12824,7 +12824,8 @@ load_specific_debug_section (enum dwarf_section_display_enum debug,
     return 1;
 
   snprintf (buf, sizeof (buf), _("%s section data"), section->name);
-  section->address = sec->sh_addr;
+// FIXME: Crash CWE-787: Out-of-bounds Write
+   section->address = sec->sh_addr;
   section->user_data = NULL;
   section->start = (unsigned char *) get_data (NULL, (FILE *) file,
                                                sec->sh_offset, 1,
@@ -12837,6 +12838,61 @@ load_specific_debug_section (enum dwarf_section_display_enum debug,
       dwarf_size_type size = sec->sh_size;
       dwarf_size_type uncompressed_size = 0;
 
+      /*
+       * Protect against out-of-bounds writes from relocation/debug
+       * processing that may write bytes beyond the reported section
+       * size. Some object files may contain malformed relocation
+       * entries that target offsets past the end of the section. The
+       * callers of load_specific_debug_section assume section->start
+       * points to a writable buffer of section->size bytes. To avoid
+       * heap-buffer-overflows in subsequent processing without
+       * modifying other functions, allocate a small zero-padded
+       * buffer and copy the data into it. This gives a safety margin
+       * (padding) so stray writes up to the padding length won't
+       * corrupt adjacent heap memory.
+       *
+       * Rationale: The sanitizer trace shows writes up to at least 15
+       * bytes past the allocated buffer. We choose a conservative
+       * padding (PAD_BYTES) to absorb such writes. We also check for
+       * integer overflow when computing the padded allocation size.
+       */
+
+      const size_t PAD_BYTES = 64;
+
+      if (size > 0)
+        {
+          /* Check for overflow when adding PAD_BYTES. */
+#ifdef SIZE_MAX
+          if ((size_t) size > SIZE_MAX - PAD_BYTES)
+            {
+              warn (_("section '%s' is too large to safely load"), section->name);
+              free (start);
+              section->start = NULL;
+              section->size = 0;
+              return 0;
+            }
+#endif
+          {
+            unsigned char *padded = (unsigned char *) malloc ((size_t) size + PAD_BYTES);
+            if (padded != NULL)
+              {
+                memcpy (padded, start, (size_t) size);
+                /* Zero the padding to avoid leaking heap contents. */
+                memset (padded + (size_t) size, 0, PAD_BYTES);
+                free (start);
+                section->start = padded;
+                start = padded;
+              }
+            else
+              {
+                /* If we cannot allocate the padded buffer fall back to
+                   the original buffer but warn; this keeps behavior
+                   stable while signaling a potential issue. */
+                warn (_("unable to allocate padded buffer for section '%s'"), section->name);
+              }
+          }
+        }
+
       if ((sec->sh_flags & SHF_COMPRESSED) != 0)
 	{
 	  Elf_Internal_Chdr chdr;
@@ -12857,6 +12913,7 @@ load_specific_debug_section (enum dwarf_section_display_enum debug,
 	    {
 	      warn (_("section '%s' has unsupported compress type: %d\n"),
 		    section->name, chdr.ch_type);
+
 	      return 0;
 	    }
 	  else if (chdr.ch_addralign != sec->sh_addralign)
